{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "082f53b2-fd8c-4383-b279-040c9dec4198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T14:59:56.382369Z",
     "iopub.status.busy": "2025-07-02T14:59:56.381621Z",
     "iopub.status.idle": "2025-07-02T14:59:56.395878Z",
     "shell.execute_reply": "2025-07-02T14:59:56.395073Z",
     "shell.execute_reply.started": "2025-07-02T14:59:56.382331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sangeethgeorge/MyProjects/oncoai-patient-outcome-navigator'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8aba01-8278-4287-8891-f807eb90be4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T15:00:52.403862Z",
     "iopub.status.busy": "2025-07-02T15:00:52.402830Z",
     "iopub.status.idle": "2025-07-02T15:00:53.120542Z",
     "shell.execute_reply": "2025-07-02T15:00:53.119922Z",
     "shell.execute_reply.started": "2025-07-02T15:00:52.403810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLflow Run with ID: eafc352555634fb1885751da6afba93b\n",
      "SHAP plots will be saved to: /Users/sangeethgeorge/MyProjects/oncoai-patient-outcome-navigator/shap_plots/eafc352555634fb1885751da6afba93b\n",
      "Model will be saved to: /Users/sangeethgeorge/MyProjects/oncoai-patient-outcome-navigator/models/logreg_model_run_eafc352555634fb1885751da6afba93b.joblib\n",
      "X_train shape after split: (3264, 13)\n",
      "X_test shape after split: (817, 13)\n",
      "\n",
      "One-hot encoding completed and columns aligned.\n",
      "X_train_ohe shape: (3264, 226)\n",
      "X_test_ohe shape: (817, 226)\n",
      "\n",
      "Leakage check completed and columns adjusted.\n",
      "X_train_leakage_checked shape: (3264, 226)\n",
      "X_test_leakage_checked shape: (817, 226)\n"
     ]
    },
    {
     "ename": "DTypePromotionError",
     "evalue": "The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDTypePromotionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 253\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# 5. Scale features\u001b[39;00m\n\u001b[32m    252\u001b[39m scaler = StandardScaler()\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m X_train_scaled = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_leakage_checked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m X_test_scaled = scaler.transform(X_test_leakage_checked)\n\u001b[32m    256\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Features prepared (one-hot encoded and scaled).\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/oncoai-prototype-GJlvEr5Z-py3.11/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/oncoai-prototype-GJlvEr5Z-py3.11/lib/python3.11/site-packages/sklearn/base.py:892\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    877\u001b[39m         warnings.warn(\n\u001b[32m    878\u001b[39m             (\n\u001b[32m    879\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    887\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    888\u001b[39m         )\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    891\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m892\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    894\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    895\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/oncoai-prototype-GJlvEr5Z-py3.11/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:907\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/oncoai-prototype-GJlvEr5Z-py3.11/lib/python3.11/site-packages/sklearn/base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/oncoai-prototype-GJlvEr5Z-py3.11/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:943\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    911\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[32m    912\u001b[39m \n\u001b[32m    913\u001b[39m \u001b[33;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m \u001b[33;03m    Fitted scaler.\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    942\u001b[39m first_call = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/oncoai-prototype-GJlvEr5Z-py3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/oncoai-prototype-GJlvEr5Z-py3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:929\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    925\u001b[39m pandas_requires_conversion = \u001b[38;5;28many\u001b[39m(\n\u001b[32m    926\u001b[39m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[32m    927\u001b[39m )\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np.dtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     dtype_orig = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdtypes_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[32m    931\u001b[39m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[32m    932\u001b[39m     dtype_orig = \u001b[38;5;28mobject\u001b[39m\n",
      "\u001b[31mDTypePromotionError\u001b[39m: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>, <class 'numpy.dtypes.BoolDType'>)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the root directory of your project.\n",
    "# This assumes the script is run from a location where 'data/', 'models/', 'shap_plots/'\n",
    "# can be found relative to this project_root.\n",
    "project_root = '/Users/sangeethgeorge/MyProjects/oncoai-patient-outcome-navigator'\n",
    "\n",
    "# Define paths for data, models, and SHAP plots\n",
    "data_file_path = os.path.join(project_root, \"data\", \"onco_features_cleaned.parquet\")\n",
    "model_save_base_path = os.path.join(project_root, \"models\")\n",
    "shap_plots_base_path = os.path.join(project_root, \"shap_plots\")\n",
    "mlruns_path = os.path.join(project_root, \"mlruns\")\n",
    "\n",
    "# Ensure necessary directories exist\n",
    "os.makedirs(os.path.dirname(data_file_path), exist_ok=True)\n",
    "os.makedirs(model_save_base_path, exist_ok=True)\n",
    "os.makedirs(shap_plots_base_path, exist_ok=True)\n",
    "os.makedirs(mlruns_path, exist_ok=True) \n",
    "\n",
    "\n",
    "# --- Function Definitions (remain mostly the same, with minor adjustments for MLflow) ---\n",
    "\n",
    "def load_dataset(path=\"/Users/sangeethgeorge/MyProjects/oncoai-patient-outcome-navigator/data/onco_features_cleaned.parquet\"):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the specified path.\n",
    "    If the file doesn't exist, a dummy dataset is created for demonstration.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Creating a dummy dataset at {path} for demonstration.\")\n",
    "        # Create a small dummy DataFrame with some numerical and categorical data\n",
    "        data = {\n",
    "            'icustay_id': range(100),\n",
    "            'subject_id': range(100, 200),\n",
    "            'hadm_id': range(200, 300),\n",
    "            'admittime': pd.to_datetime(['2020-01-01'] * 100),\n",
    "            'dob': pd.to_datetime(['1980-01-01'] * 100),\n",
    "            'dod': pd.to_datetime(['2020-02-01'] * 100),\n",
    "            'feature_A': np.random.rand(100),\n",
    "            'feature_B': np.random.randint(0, 10, 100),\n",
    "            'categorical_C': np.random.choice(['X', 'Y', 'Z'], 100),\n",
    "            'mortality_30d': np.random.randint(0, 2, 100)\n",
    "        }\n",
    "        dummy_df = pd.DataFrame(data)\n",
    "        dummy_df.to_parquet(path)\n",
    "        print(\"Dummy dataset created.\")\n",
    "    df = pd.read_parquet(path)\n",
    "    return df\n",
    "\n",
    "def train_test_impute_split(df, label_col=\"mortality_30d\"):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets, and imputes missing values\n",
    "    using the median from the training set.\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=['icustay_id', 'subject_id', 'hadm_id', 'admittime', 'dob', 'dod'], errors='ignore')\n",
    "    y = df[label_col]\n",
    "    X = df.drop(columns=[label_col])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    train_median = X_train.median(numeric_only=True)\n",
    "    X_train = X_train.fillna(train_median)\n",
    "    X_test = X_test.fillna(train_median)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def check_for_leakage(X, y):\n",
    "    \"\"\"\n",
    "    Checks for potential data leakage by identifying highly correlated features\n",
    "    with the target variable and removes them.\n",
    "    \"\"\"\n",
    "    corr = pd.concat([X, y], axis=1).corr(numeric_only=True)[y.name].drop(y.name)\n",
    "    high_corr = corr[abs(corr) > 0.95]\n",
    "    if not high_corr.empty:\n",
    "        print(\"\\nâš ï¸ Potential Leakage Detected:\")\n",
    "        print(high_corr)\n",
    "        X = X.drop(columns=high_corr.index)\n",
    "    return X\n",
    "\n",
    "# Modified to return y_pred and y_prob for logging metrics\n",
    "def train_logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Trains a Logistic Regression model and evaluates its performance.\n",
    "    Returns the model, original (scaled) X_train/X_test, y_train/y_test,\n",
    "    and predictions/probabilities for logging.\n",
    "    \"\"\"\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(\"\\nðŸ§  Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nðŸ“Š ROC AUC Score:\", roc_auc_score(y_test, y_prob))\n",
    "\n",
    "    return model, X_train, y_train, X_test, y_test, y_pred, y_prob\n",
    "\n",
    "# Renamed and adjusted to fit the MLflow flow, taking X_df for feature names\n",
    "def explain_predictions(model, X_scaled_for_shap, X_df_for_shap, output_dir=\"shap_plots\", top_n=10):\n",
    "    \"\"\"\n",
    "    Generates SHAP plots to explain the model's predictions.\n",
    "    Saves plots to the specified output directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if X_scaled_for_shap.shape[0] == 0:\n",
    "        print(\"Skipping SHAP explanation: No data in X_scaled_for_shap for explanation.\")\n",
    "        return\n",
    "\n",
    "    # Use the original DataFrame (X_df_for_shap) for feature names and display data\n",
    "    explainer = shap.Explainer(model, X_scaled_for_shap, feature_names=X_df_for_shap.columns.tolist())\n",
    "    shap_values = explainer(X_scaled_for_shap)\n",
    "    shap_values.feature_names = X_df_for_shap.columns.tolist() # Ensure feature names are set\n",
    "\n",
    "    # Assign display_data to shap_values object (important for some SHAP plots)\n",
    "    # Ensure display_data has the same number of rows as shap_values.values\n",
    "    if shap_values.values.shape[0] == X_df_for_shap.shape[0]:\n",
    "        shap_values.display_data = X_df_for_shap.values\n",
    "    else:\n",
    "        print(\"Warning: Mismatch in row count between shap_values and X_df_for_shap. display_data not set.\")\n",
    "\n",
    "\n",
    "    print(\"\\nðŸ“ˆ Generating overall SHAP Summary Plot...\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, features=X_df_for_shap, show=False) # Use X_df_for_shap for features\n",
    "    plt.savefig(os.path.join(output_dir, \"shap_summary_overall.png\"), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"âœ… Overall SHAP Summary Plot generated.\")\n",
    "\n",
    "    abs_shap_means = np.abs(shap_values.values).mean(axis=0)\n",
    "    # Get top 3 overall features for dependence plots\n",
    "    top_overall_features_indices = np.argsort(abs_shap_means)[::-1][:min(3, len(X_df_for_shap.columns))]\n",
    "    for feat_idx in top_overall_features_indices:\n",
    "        top_feature_name = X_df_for_shap.columns[feat_idx]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        # Use X_df_for_shap for features argument in dependence_plot\n",
    "        shap.dependence_plot(top_feature_name, shap_values.values, features=X_df_for_shap, feature_names=X_df_for_shap.columns.tolist(), show=False)\n",
    "        plt.title(f\"SHAP Dependence Plot: {top_feature_name}\")\n",
    "        plt.savefig(os.path.join(output_dir, f\"shap_dependence_{top_feature_name}.png\"), bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    risk_scores = model.predict_proba(X_scaled_for_shap)[:, 1]\n",
    "    actual_top_n = min(top_n, X_scaled_for_shap.shape[0])\n",
    "    top_indices = np.argsort(risk_scores)[-actual_top_n:][::-1] # Indices of top N highest risk patients\n",
    "\n",
    "    for i_idx, original_index in enumerate(top_indices):\n",
    "        # Original index refers to the index in the X_scaled_for_shap array\n",
    "        patient_identifier = f\"patient_idx_{original_index}_rank_{i_idx+1}\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        shap.plots.waterfall(shap_values[original_index], show=False)\n",
    "        plt.title(f\"SHAP Waterfall Plot for {patient_identifier}\\nPredicted Risk: {risk_scores[original_index]:.4f}\")\n",
    "        plt.savefig(os.path.join(output_dir, f\"waterfall_{patient_identifier}.png\"), bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # For the top N patients, generate dependence plots for their top 3 contributing features\n",
    "        relevant_shap_values_patient = shap_values.values[original_index]\n",
    "        top_patient_features_indices = np.argsort(np.abs(relevant_shap_values_patient))[::-1][:min(3, len(X_df_for_shap.columns))]\n",
    "\n",
    "        for feat_idx in top_patient_features_indices:\n",
    "            feat_name = X_df_for_shap.columns[feat_idx]\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            shap.dependence_plot(feat_name, shap_values.values, features=X_df_for_shap, feature_names=X_df_for_shap.columns.tolist(), show=False)\n",
    "            plt.title(f\"SHAP Dependence Plot for {patient_identifier} - {feat_name}\")\n",
    "            plt.savefig(os.path.join(output_dir, f\"dependence_{patient_identifier}_{feat_name}.png\"), bbox_inches='tight')\n",
    "            plt.close()\n",
    "    print(\"âœ… All SHAP plots generated and saved.\")\n",
    "\n",
    "def save_model(model, scaler, output_path):\n",
    "    \"\"\"\n",
    "    Saves the trained model and scaler to a joblib file.\n",
    "    \"\"\"\n",
    "    joblib.dump({\"model\": model, \"scaler\": scaler}, output_path)\n",
    "    print(f\"\\nâœ… Saved model and scaler to {output_path}\")\n",
    "\n",
    "# --- Jupyter Notebook Execution Flow ---\n",
    "\n",
    "# Set up MLflow tracking\n",
    "mlflow.set_experiment(\"OncoAI-Mortality-Prediction\")\n",
    "\n",
    "# Start an MLflow run\n",
    "# The 'with' statement ensures the run is properly ended, even if errors occur.\n",
    "with mlflow.start_run() as run:\n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    # Make SHAP plots output directory specific to the run\n",
    "    run_shap_output_dir = os.path.join(shap_plots_base_path, run_id)\n",
    "    os.makedirs(run_shap_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Define the full model save path for this run\n",
    "    model_save_path_for_run = os.path.join(model_save_base_path, f\"logreg_model_run_{run_id}.joblib\")\n",
    "\n",
    "    print(f\"Starting MLflow Run with ID: {run_id}\")\n",
    "    print(f\"SHAP plots will be saved to: {run_shap_output_dir}\")\n",
    "    print(f\"Model will be saved to: {model_save_path_for_run}\")\n",
    "\n",
    "    # Log parameters to MLflow (common setup parameters)\n",
    "    mlflow.log_param(\"test_size\", 0.2)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"stratify\", \"mortality_30d\")\n",
    "    mlflow.log_param(\"logistic_regression_max_iter\", 1000)\n",
    "\n",
    "    # 1. Load dataset\n",
    "    df = load_dataset()\n",
    "\n",
    "    if df.empty: # Check if DataFrame is empty\n",
    "        print(\"âŒ Dataset is empty. Cannot proceed with training and explanation. Exiting MLflow run.\")\n",
    "        mlflow.end_run(status=\"FAILED\")\n",
    "    else:\n",
    "        # 2. Train-test split and imputation\n",
    "        X_train, X_test, y_train, y_test = train_test_impute_split(df)\n",
    "        print(f\"X_train shape after split: {X_train.shape}\")\n",
    "        print(f\"X_test shape after split: {X_test.shape}\")\n",
    "\n",
    "        # 3. One-hot encode categorical columns\n",
    "        X_train_ohe = pd.get_dummies(X_train, drop_first=True)\n",
    "        X_test_ohe = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "        # Align columns after one-hot encoding to ensure same features in train/test\n",
    "        missing_cols_in_test = set(X_train_ohe.columns) - set(X_test_ohe.columns)\n",
    "        for c in missing_cols_in_test:\n",
    "            X_test_ohe[c] = 0\n",
    "        # Ensure the order of columns is the same\n",
    "        X_test_ohe = X_test_ohe[X_train_ohe.columns]\n",
    "        print(\"\\nOne-hot encoding completed and columns aligned.\")\n",
    "        print(f\"X_train_ohe shape: {X_train_ohe.shape}\")\n",
    "        print(f\"X_test_ohe shape: {X_test_ohe.shape}\")\n",
    "\n",
    "        # 4. Check for data leakage on the one-hot encoded training data\n",
    "        X_train_leakage_checked = check_for_leakage(X_train_ohe, y_train)\n",
    "\n",
    "        # Apply the same column selection (after leakage check) to the test set\n",
    "        X_test_leakage_checked = X_test_ohe[X_train_leakage_checked.columns]\n",
    "        print(\"\\nLeakage check completed and columns adjusted.\")\n",
    "        print(f\"X_train_leakage_checked shape: {X_train_leakage_checked.shape}\")\n",
    "        print(f\"X_test_leakage_checked shape: {X_test_leakage_checked.shape}\")\n",
    "\n",
    "\n",
    "        # 5. Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_leakage_checked)\n",
    "        X_test_scaled = scaler.transform(X_test_leakage_checked)\n",
    "\n",
    "        print(\"âœ… Features prepared (one-hot encoded and scaled).\")\n",
    "\n",
    "        # Log parameters to MLflow\n",
    "        mlflow.log_param(\"scaler_type\", \"StandardScaler\")\n",
    "        mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "        mlflow.log_param(\"num_features_after_preprocessing\", X_train_scaled.shape[1])\n",
    "\n",
    "\n",
    "        # 6. Train Logistic Regression model\n",
    "        # The train_logistic_regression function now returns y_pred and y_prob\n",
    "        model, X_train_final_scaled, y_train_final, X_test_final_scaled, y_test_final, y_pred, y_prob = \\\n",
    "            train_logistic_regression(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "        # Log ROC AUC metric to MLflow\n",
    "        auc = roc_auc_score(y_test_final, y_prob)\n",
    "        mlflow.log_metric(\"roc_auc\", auc)\n",
    "        print(f\"Logged ROC AUC to MLflow: {auc:.4f}\")\n",
    "\n",
    "        # You can also log other metrics from the classification report if desired\n",
    "        # For example, to log precision, recall, f1-score for class 1 (positive class)\n",
    "        report = classification_report(y_test_final, y_pred, output_dict=True)\n",
    "        mlflow.log_metric(\"precision_1\", report['1']['precision'])\n",
    "        mlflow.log_metric(\"recall_1\", report['1']['recall'])\n",
    "        mlflow.log_metric(\"f1_score_1\", report['1']['f1-score'])\n",
    "        mlflow.log_metric(\"accuracy\", report['accuracy'])\n",
    "\n",
    "\n",
    "        # 7. Save model and scaler\n",
    "        save_model(model, scaler, output_path=model_save_path_for_run)\n",
    "\n",
    "        # 8. Log model with input_example for signature inference\n",
    "        # Use X_train_leakage_checked (DataFrame) for input_example as it preserves column names\n",
    "        if X_train_leakage_checked.shape[0] > 0:\n",
    "            mlflow.sklearn.log_model(model, \"logreg_model\",\n",
    "                                     input_example=X_train_leakage_checked.head(5)) # using .head(5) for a smaller example\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, \"logreg_model\")\n",
    "        print(\"Model logged to MLflow with signature.\")\n",
    "\n",
    "\n",
    "        # 9. Explain with SHAP\n",
    "        # Pass X_test_scaled (numpy array) and X_test_leakage_checked (DataFrame)\n",
    "        # X_test_leakage_checked is critical for feature names and display data for SHAP.\n",
    "        explain_predictions(model, X_test_scaled, X_test_leakage_checked, output_dir=run_shap_output_dir)\n",
    "        print(\"\\nSHAP explanations generated and plots saved.\")\n",
    "\n",
    "        # 10. Log SHAP plots as MLflow artifacts\n",
    "        shap_plot_files = [f for f in os.listdir(run_shap_output_dir) if f.endswith('.png')]\n",
    "        for plot_file in shap_plot_files:\n",
    "            mlflow.log_artifact(os.path.join(run_shap_output_dir, plot_file), artifact_path=\"shap_plots\")\n",
    "        print(f\"Logged {len(shap_plot_files)} SHAP plots as MLflow artifacts.\")\n",
    "\n",
    "print(\"\\nâœ¨ MLflow run completed successfully. Check your MLflow UI for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f0a90-f383-4496-a9f4-8bb4e0f96b04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
