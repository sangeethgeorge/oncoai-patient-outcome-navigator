{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6124499a-afb2-41c7-86ff-68fd84ed9a1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T14:20:45.595303Z",
     "iopub.status.busy": "2025-07-02T14:20:45.595218Z",
     "iopub.status.idle": "2025-07-02T14:20:45.663112Z",
     "shell.execute_reply": "2025-07-02T14:20:45.662675Z",
     "shell.execute_reply.started": "2025-07-02T14:20:45.595294Z"
    }
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the Postgres extension\n",
    "duckdb.sql(\"INSTALL postgres_scanner;\")\n",
    "duckdb.sql(\"LOAD postgres_scanner;\")\n",
    "\n",
    "# credentials\n",
    "pg_conn_str = \"dbname='mimic-iii' user='sangeethgeorge' password='12345' host='localhost' port='5432'\"\n",
    "\n",
    "#SQL query to connect to DB\n",
    "labs48_query = \"\"\"\n",
    "SELECT *\n",
    "FROM postgres_scan('dbname=mimic-iii user=sangeethgeorge password=12345 host=localhost port=5432', \n",
    "                   'public', 'all_labs_48h')\n",
    "\"\"\"\n",
    "vitals48_query = \"\"\"\n",
    "SELECT *\n",
    "FROM postgres_scan('dbname=mimic-iii user=sangeethgeorge password=12345 host=localhost port=5432', \n",
    "                   'public', 'all_vitals_48h')\n",
    "\"\"\"\n",
    "\n",
    "onc_cohort = \"\"\"\n",
    "SELECT *\n",
    "FROM postgres_scan('dbname=mimic-iii user=sangeethgeorge password=12345 host=localhost port=5432', \n",
    "                   'public', 'oncology_icu_base')\n",
    "\"\"\"\n",
    "\n",
    "labs48_df= duckdb.sql(labs48_query).df()\n",
    "vitals48_df = duckdb.sql(vitals48_query).df()\n",
    "onc_cohort_df = duckdb.sql(onc_cohort).df()\n",
    "\n",
    "labs48_df\n",
    "\n",
    "vitals48_df\n",
    "\n",
    "onc_cohort_df.columns\n",
    "\n",
    "# Clean & Prepare\n",
    "labs48_df['charttime'] = pd.to_datetime(labs48_df['charttime'])\n",
    "labs48_df.dropna(subset=['labs_valuenum'], inplace=True)\n",
    "\n",
    "# Clean & Prepare\n",
    "vitals48_df['charttime'] = pd.to_datetime(vitals48_df['charttime'])\n",
    "vitals48_df.dropna(subset=['vitals_valuenum'], inplace=True)\n",
    "\n",
    "\n",
    "# --- High Coverage Vitals ---\n",
    "\n",
    "# 1. Calculate the coverage for each vital label (percentage of unique ICU stays it appears in)\n",
    "total_unique_stays = vitals48_df['icustay_id'].nunique()\n",
    "vitals48_coverage = vitals48_df.groupby('vitals_label')['icustay_id'].nunique() / total_unique_stays\n",
    "\n",
    "# 2. Define your high coverage threshold (e.g., 0.8 for 80%)\n",
    "coverage_threshold = 0.7\n",
    "\n",
    "# 3. Filter to get only the labels that meet the high coverage threshold\n",
    "high_coverage_labels = vitals48_coverage[vitals48_coverage >= coverage_threshold].index.tolist()\n",
    "\n",
    "print(\"Labels with High Coverage (>= {}%):\".format(coverage_threshold * 100))\n",
    "print(high_coverage_labels)\n",
    "\n",
    "# 4. Filter the original DataFrame to include only these high-coverage labels\n",
    "filtered_vitals_df_high_coverage = vitals48_df[vitals48_df['vitals_label'].isin(high_coverage_labels)]\n",
    "\n",
    "# 5. Calculate unique ICU stays for each of the high-coverage labels\n",
    "unique_stays_per_high_coverage_label = filtered_vitals_df_high_coverage.groupby('vitals_label')['icustay_id'].nunique()\n",
    "\n",
    "# 6. Calculate the percentage of unique ICU stays for each high-coverage label\n",
    "#    (This is essentially `vitals48_coverage` for the selected labels, but we recalculate\n",
    "#     it here to fit the structure of the original code's output)\n",
    "percentage_unique_stays_high_coverage = (unique_stays_per_high_coverage_label / total_unique_stays) * 100\n",
    "\n",
    "# 7. Combine the counts and percentages for better readability\n",
    "vitals48_high_coverage_pct_df = pd.DataFrame({\n",
    "    'Unique ICU Stays Count': unique_stays_per_high_coverage_label,\n",
    "    'Percentage of Total Unique ICU Stays': percentage_unique_stays_high_coverage\n",
    "}).sort_values(by='Percentage of Total Unique ICU Stays', ascending=False)\n",
    "\n",
    "vitals48_high_coverage_pct_df\n",
    "\n",
    "# --- High Coverage Labs ---\n",
    "\n",
    "# 1. Calculate the coverage for each vital label (percentage of unique ICU stays it appears in)\n",
    "total_unique_stays = labs48_df['icustay_id'].nunique()\n",
    "labs48_coverage = labs48_df.groupby('labs_label')['icustay_id'].nunique() / total_unique_stays\n",
    "\n",
    "# 2. Define your high coverage threshold (e.g., 0.8 for 80%)\n",
    "coverage_threshold = 0.7\n",
    "\n",
    "# 3. Filter to get only the labels that meet the high coverage threshold\n",
    "high_coverage_labels = labs48_coverage[labs48_coverage >= coverage_threshold].index.tolist()\n",
    "\n",
    "print(\"Labels with High Coverage (>= {}%):\".format(coverage_threshold * 100))\n",
    "print(high_coverage_labels)\n",
    "\n",
    "# 4. Filter the original DataFrame to include only these high-coverage labels\n",
    "filtered_labs_df_high_coverage = labs48_df[labs48_df['labs_label'].isin(high_coverage_labels)]\n",
    "\n",
    "# 5. Calculate unique ICU stays for each of the high-coverage labels\n",
    "unique_stays_per_high_coverage_label = filtered_labs_df_high_coverage.groupby('labs_label')['icustay_id'].nunique()\n",
    "\n",
    "# 6. Calculate the percentage of unique ICU stays for each high-coverage label\n",
    "#    (This is essentially `labs48_coverage` for the selected labels, but we recalculate\n",
    "#     it here to fit the structure of the original code's output)\n",
    "percentage_unique_stays_high_coverage = (unique_stays_per_high_coverage_label / total_unique_stays) * 100\n",
    "\n",
    "# 7. Combine the counts and percentages for better readability\n",
    "labs48_high_coverage_pct_df = pd.DataFrame({\n",
    "    'Unique ICU Stays Count': unique_stays_per_high_coverage_label,\n",
    "    'Percentage of Total Unique ICU Stays': percentage_unique_stays_high_coverage\n",
    "}).sort_values(by='Percentage of Total Unique ICU Stays', ascending=False)\n",
    "\n",
    "labs48_high_coverage_pct_df\n",
    "\n",
    "# --- High Coverage Vitals ---\n",
    "\n",
    "# 1. Calculate the coverage for each vital label (percentage of unique ICU stays it appears in)\n",
    "total_unique_stays = vitals48_df['icustay_id'].nunique()\n",
    "vitals48_coverage = vitals48_df.groupby('vitals_label')['icustay_id'].nunique() / total_unique_stays\n",
    "\n",
    "# 2. Define your high coverage threshold (e.g., 0.8 for 80%)\n",
    "coverage_threshold = 0.95\n",
    "\n",
    "# 3. Filter to get only the labels that meet the high coverage threshold\n",
    "high_coverage_labels = vitals48_coverage[vitals48_coverage >= coverage_threshold].index.tolist()\n",
    "\n",
    "print(\"Labels with High Coverage (>= {}%):\".format(coverage_threshold * 100))\n",
    "print(high_coverage_labels)\n",
    "\n",
    "# 4. Filter the original DataFrame to include only these high-coverage labels\n",
    "filtered_vitals_df_high_coverage = vitals48_df[vitals48_df['vitals_label'].isin(high_coverage_labels)]\n",
    "\n",
    "# 5. Calculate unique ICU stays for each of the high-coverage labels\n",
    "unique_stays_per_high_coverage_label = filtered_vitals_df_high_coverage.groupby('vitals_label')['icustay_id'].nunique()\n",
    "\n",
    "# 6. Calculate the percentage of unique ICU stays for each high-coverage label\n",
    "percentage_unique_stays_high_coverage = (unique_stays_per_high_coverage_label / total_unique_stays) * 100\n",
    "\n",
    "# 7. Combine the counts and percentages for better readability\n",
    "vitals48_high_coverage_pct_df = pd.DataFrame({\n",
    "    'Unique ICU Stays Count': unique_stays_per_high_coverage_label,\n",
    "    'Percentage of Total Unique ICU Stays': percentage_unique_stays_high_coverage\n",
    "}).sort_values(by='Percentage of Total Unique ICU Stays', ascending=False)\n",
    "\n",
    "vitals48_high_coverage_pct_df\n",
    "\n",
    "# Start from your filtered long-form vitals\n",
    "df = filtered_vitals_df_high_coverage.copy()\n",
    "\n",
    "# Convert charttime to datetime if not already (redundant if already done, but safe)\n",
    "df['charttime'] = pd.to_datetime(df['charttime'])\n",
    "\n",
    "# Feature engineering function per vital per ICU stay\n",
    "def compute_time_series_features(group):\n",
    "    # Ensure times and values are numpy arrays for reshape\n",
    "    times = (group['charttime'] - group['charttime'].min()).dt.total_seconds().values / 3600.0  # in hours\n",
    "    values = group['vitals_valuenum'].values\n",
    "\n",
    "    # Defensive check\n",
    "    if len(values) < 1:\n",
    "        return pd.Series({'mean': np.nan, 'min': np.nan, 'max': np.nan, 'slope': np.nan})\n",
    "\n",
    "    # Compute features\n",
    "    features = {\n",
    "        'mean': np.mean(values),\n",
    "        'min': np.min(values),\n",
    "        'max': np.max(values),\n",
    "    }\n",
    "\n",
    "    # Linear regression slope over time\n",
    "    if len(values) > 1:\n",
    "        # Use .reshape(-1, 1) on the NumPy array obtained from .values\n",
    "        model = LinearRegression().fit(times.reshape(-1, 1), values)\n",
    "        features['slope'] = model.coef_[0]\n",
    "    else:\n",
    "        features['slope'] = np.nan\n",
    "\n",
    "    return pd.Series(features)\n",
    "\n",
    "# Group by ICU stay and label → apply function\n",
    "grouped = df.groupby(['icustay_id', 'vitals_label'])\n",
    "vitals_features_df = grouped.apply(compute_time_series_features, include_groups=False).reset_index()\n",
    "# Pivot to wide format\n",
    "vitals_features_wide = vitals_features_df.pivot(index='icustay_id', columns='vitals_label')\n",
    "vitals_features_wide.columns = [f\"{stat.lower()}_{label.lower().replace(' ', '_')}\" for stat, label in vitals_features_wide.columns]\n",
    "vitals_features_wide.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "vitals_features_wide\n",
    "\n",
    "\n",
    "# --- High Coverage labs ---\n",
    "\n",
    "# 1. Calculate the coverage for each vital label (percentage of unique ICU stays it appears in)\n",
    "total_unique_stays = labs48_df['icustay_id'].nunique()\n",
    "labs48_coverage = labs48_df.groupby('labs_label')['icustay_id'].nunique() / total_unique_stays\n",
    "\n",
    "# 2. Define your high coverage threshold (e.g., 0.8 for 80%)\n",
    "coverage_threshold = 0.95\n",
    "\n",
    "# 3. Filter to get only the labels that meet the high coverage threshold\n",
    "high_coverage_labels = labs48_coverage[labs48_coverage >= coverage_threshold].index.tolist()\n",
    "\n",
    "print(\"Labels with High Coverage (>= {}%):\".format(coverage_threshold * 100))\n",
    "print(high_coverage_labels)\n",
    "\n",
    "# 4. Filter the original DataFrame to include only these high-coverage labels\n",
    "filtered_labs_df_high_coverage = labs48_df[labs48_df['labs_label'].isin(high_coverage_labels)]\n",
    "\n",
    "# 5. Calculate unique ICU stays for each of the high-coverage labels\n",
    "unique_stays_per_high_coverage_label = filtered_labs_df_high_coverage.groupby('labs_label')['icustay_id'].nunique()\n",
    "\n",
    "# 6. Calculate the percentage of unique ICU stays for each high-coverage label\n",
    "percentage_unique_stays_high_coverage = (unique_stays_per_high_coverage_label / total_unique_stays) * 100\n",
    "\n",
    "# 7. Combine the counts and percentages for better readability\n",
    "labs48_high_coverage_pct_df = pd.DataFrame({\n",
    "    'Unique ICU Stays Count': unique_stays_per_high_coverage_label,\n",
    "    'Percentage of Total Unique ICU Stays': percentage_unique_stays_high_coverage\n",
    "}).sort_values(by='Percentage of Total Unique ICU Stays', ascending=False)\n",
    "\n",
    "labs48_high_coverage_pct_df\n",
    "\n",
    "# Start from your filtered long-form labs\n",
    "df = filtered_labs_df_high_coverage.copy()\n",
    "\n",
    "# Convert charttime to datetime if not already (redundant if already done, but safe)\n",
    "df['charttime'] = pd.to_datetime(df['charttime'])\n",
    "\n",
    "# Feature engineering function per vital per ICU stay\n",
    "def compute_time_series_features(group):\n",
    "    # Ensure times and values are numpy arrays for reshape\n",
    "    times = (group['charttime'] - group['charttime'].min()).dt.total_seconds().values / 3600.0  # in hours\n",
    "    values = group['labs_valuenum'].values\n",
    "\n",
    "    # Defensive check\n",
    "    if len(values) < 1:\n",
    "        return pd.Series({'mean': np.nan, 'min': np.nan, 'max': np.nan, 'slope': np.nan})\n",
    "\n",
    "    # Compute features\n",
    "    features = {\n",
    "        'mean': np.mean(values),\n",
    "        'min': np.min(values),\n",
    "        'max': np.max(values),\n",
    "    }\n",
    "\n",
    "    # Linear regression slope over time\n",
    "    if len(values) > 1:\n",
    "        # Use .reshape(-1, 1) on the NumPy array obtained from .values\n",
    "        model = LinearRegression().fit(times.reshape(-1, 1), values)\n",
    "        features['slope'] = model.coef_[0]\n",
    "    else:\n",
    "        features['slope'] = np.nan\n",
    "\n",
    "    return pd.Series(features)\n",
    "\n",
    "# Group by ICU stay and label → apply function\n",
    "grouped = df.groupby(['icustay_id', 'labs_label'])\n",
    "labs_features_df = grouped.apply(compute_time_series_features, include_groups=False).reset_index()\n",
    "# Pivot to wide format\n",
    "labs_features_wide = labs_features_df.pivot(index='icustay_id', columns='labs_label')\n",
    "labs_features_wide.columns = [f\"{stat.lower()}_{label.lower().replace(' ', '_')}\" for stat, label in labs_features_wide.columns]\n",
    "labs_features_wide.reset_index(inplace=True)\n",
    "\n",
    "labs_features_wide\n",
    "\n",
    "\n",
    "# Merge vitals\n",
    "onco_feature_df = onc_cohort_df.merge(vitals_features_wide, on='icustay_id', how='left')\n",
    "\n",
    "# Merge labs\n",
    "onco_feature_df = onco_feature_df.merge(labs_features_wide, on='icustay_id', how='left')\n",
    "\n",
    "onco_feature_df\n",
    "\n",
    "\n",
    "# 1. Calculate the percentage of non-null values for each column\n",
    "completeness = onco_feature_df.count() / len(onco_feature_df)\n",
    "\n",
    "# 2. Define a threshold for \"most of its rows filled\"\n",
    "completeness_threshold = 0.95\n",
    "\n",
    "# 3. Filter columns based on the completeness threshold\n",
    "high_completeness_columns = completeness[completeness >= completeness_threshold].index.tolist()\n",
    "\n",
    "print(f\"Columns with at least {completeness_threshold*100}% of rows filled:\")\n",
    "print(high_completeness_columns)\n",
    "\n",
    "# 4. Select these columns from the original DataFrame\n",
    "onco_features_high_completeness = onco_feature_df[high_completeness_columns]\n",
    "\n",
    "onco_features_high_completeness\n",
    "\n",
    "\n",
    "# percentage or an absolute number of non-NaN values.\n",
    "percentage_filled_threshold = 0.95 # Keep rows that are at least 75% filled\n",
    "\n",
    "# Calculate the minimum number of non-NaN values required based on the percentage\n",
    "num_columns = onco_features_high_completeness.shape[1]\n",
    "min_non_nan_for_row = int(num_columns * percentage_filled_threshold)\n",
    "\n",
    "print(f\"Keeping rows with at least {min_non_nan_for_row} non-NaN values (i.e., {percentage_filled_threshold*100}% filled).\")\n",
    "\n",
    "# Drop rows using the calculated threshold\n",
    "onco_cohort_ML = onco_features_high_completeness.dropna(\n",
    "    axis=0,        # Operate on rows (default, but good to be explicit)\n",
    "    thresh=min_non_nan_for_row\n",
    ").copy()\n",
    "\n",
    "print(\"\\nDataFrame after dropping rows that are less than {}% filled:\".format(percentage_filled_threshold*100))\n",
    "onco_cohort_ML\n",
    "\n",
    "# Optional: Impute all missing values (safely: only numeric)\n",
    "numeric_cols = onco_cohort_ML.select_dtypes(include='number').columns.drop('mortality_30d')\n",
    "\n",
    "onco_cohort_ML[numeric_cols] = onco_cohort_ML[numeric_cols].fillna(onco_cohort_ML[numeric_cols].median())\n",
    "onco_cohort_ML\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "# Load and prepare data\n",
    "onco_cohort_ML \n",
    "X = onco_cohort_ML.drop(columns=['mortality_30d'] + columns_to_drop_from_features, errors='ignore')\n",
    "y = onco_cohort_ML['mortality_30d']\n",
    "\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit XGBoost\n",
    "model = xgb.XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# SHAP explanation\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_test, max_display=10)\n",
    "\n",
    "# Feature importance (mean abs SHAP)\n",
    "shap_df = pd.DataFrame(shap_values.values, columns=X.columns)\n",
    "mean_shap = shap_df.abs().mean().sort_values(ascending=False)\n",
    "print(\"🔝 Top 10 impactful features:\")\n",
    "print(mean_shap.head(10))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mean_shap.head(10)\n",
    "\n",
    "top_10_features = mean_shap.head(10).index.tolist()\n",
    "\n",
    "missing_cols = [col for col in top_10_features if col not in onco_cohort_ML.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(\"⚠️ These top SHAP features were not found in the original DataFrame:\")\n",
    "    print(missing_cols)\n",
    "\n",
    "\n",
    "# Combine identifiers, top SHAP features, and target\n",
    "all_selected_columns = columns_to_drop_from_features + top_10_features + ['mortality_30d']\n",
    "\n",
    "# Filter to only columns that exist in the DataFrame\n",
    "valid_columns = [col for col in all_selected_columns if col in onco_cohort_ML.columns]\n",
    "\n",
    "# Create the final subset DataFrame\n",
    "final_feature_subset = onco_cohort_ML[valid_columns].copy()\n",
    "\n",
    "# Confirm result\n",
    "print(f\"✅ Saved {final_feature_subset.shape[0]} rows × {final_feature_subset.shape[1]} columns\")\n",
    "print(\"Included columns:\", final_feature_subset.columns.tolist())\n",
    "\n",
    "\n",
    "\n",
    "# Optional: Save the subset\n",
    "final_feature_subset.to_parquet(\n",
    "    \"/Users/sangeethgeorge/MyProjects/oncoai-patient-outcome-navigator/data/onco_features_cleaned.parquet\",\n",
    "    index=False\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
