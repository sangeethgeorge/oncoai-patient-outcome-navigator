{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ce895d-9be3-4cd6-9d11-97f615dc6457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T15:35:08.046475Z",
     "iopub.status.busy": "2025-07-02T15:35:08.045564Z",
     "iopub.status.idle": "2025-07-02T15:35:12.470318Z",
     "shell.execute_reply": "2025-07-02T15:35:12.470010Z",
     "shell.execute_reply.started": "2025-07-02T15:35:08.046425Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/02 11:35:08 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLflow Run with ID: ae10757d5eb94eb681115211fb918898\n",
      "SHAP plots will be saved to: /Users/sangeethgeorge/MyProjects/oncoai-patient-outcome-navigator/shap_plots/ae10757d5eb94eb681115211fb918898\n",
      "Model will be saved to: /Users/sangeethgeorge/MyProjects/oncoai-patient-outcome-navigator/models/logreg_model_run_ae10757d5eb94eb681115211fb918898.joblib\n",
      "✅ Dataset loaded successfully from /Users/sangeethgeorge/MyProjects/oncoai-patient-outcome-navigator/data/onco_features_cleaned.parquet\n",
      "✅ Data split and imputed successfully.\n",
      "\n",
      "No significant data leakage detected based on high correlation.\n",
      "\n",
      "DEBUG: Columns of X_train_leakage_checked before scaling:\n",
      "['mean_mchc', 'min_heart_rate', 'max_bicarbonate', 'mean_urea_nitrogen', 'age', 'min_white_blood_cells', 'mean_chloride', 'max_mchc', 'min_urea_nitrogen', 'mean_glucose']\n",
      "DEBUG: Columns of X_test_leakage_checked before scaling:\n",
      "['mean_mchc', 'min_heart_rate', 'max_bicarbonate', 'mean_urea_nitrogen', 'age', 'min_white_blood_cells', 'mean_chloride', 'max_mchc', 'min_urea_nitrogen', 'mean_glucose']\n",
      "✅ Features prepared (one-hot encoded and scaled).\n",
      "\n",
      "🧠 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91       650\n",
      "           1       0.76      0.37      0.49       167\n",
      "\n",
      "    accuracy                           0.85       817\n",
      "   macro avg       0.81      0.67      0.70       817\n",
      "weighted avg       0.84      0.85      0.82       817\n",
      "\n",
      "\n",
      "📊 ROC AUC Score: 0.7912022109626901\n",
      "✅ Logistic Regression model trained and evaluated.\n",
      "\n",
      "✅ Saved model and scaler to /Users/sangeethgeorge/MyProjects/oncoai-patient-outcome-navigator/models/logreg_model_run_ae10757d5eb94eb681115211fb918898.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sangeethgeorge/Library/Caches/pypoetry/virtualenvs/oncoai-prototype-GJlvEr5Z-py3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/sangeethgeorge/Library/Caches/pypoetry/virtualenvs/oncoai-prototype-GJlvEr5Z-py3.11/lib/python3.11/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "/Users/sangeethgeorge/Library/Caches/pypoetry/virtualenvs/oncoai-prototype-GJlvEr5Z-py3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SHAP summary plot saved.\n",
      "✅ SHAP dependence plots saved.\n",
      "✅ SHAP waterfall plots for top 10 patients saved.\n",
      "✅ SHAP waterfall plot for test_patient_0 saved.\n",
      "\n",
      "✨ MLflow run completed successfully. Check your MLflow UI for details.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the root directory of your project.\n",
    "# This assumes the script is run from a location where 'data/', 'models/', 'shap_plots/'\n",
    "# can be found relative to this project_root.\n",
    "project_root = '/Users/sangeethgeorge/MyProjects/oncoai-patient-outcome-navigator'\n",
    "\n",
    "# Define paths for data, models, and SHAP plots\n",
    "data_file_path = os.path.join(project_root, \"data\", \"onco_features_cleaned.parquet\")\n",
    "model_save_base_path = os.path.join(project_root, \"models\")\n",
    "shap_plots_base_path = os.path.join(project_root, \"shap_plots\")\n",
    "mlruns_path = os.path.join(project_root, \"mlruns\")\n",
    "\n",
    "# Ensure necessary directories exist\n",
    "os.makedirs(os.path.dirname(data_file_path), exist_ok=True)\n",
    "os.makedirs(model_save_base_path, exist_ok=True)\n",
    "os.makedirs(shap_plots_base_path, exist_ok=True)\n",
    "os.makedirs(mlruns_path, exist_ok=True) \n",
    "\n",
    "# Ensure the .trash directory exists within mlruns\n",
    "os.makedirs(os.path.join(mlruns_path, \".trash\"), exist_ok=True) # ADD THIS LINE\n",
    "\n",
    "\n",
    "# --- Data Loading Function ---\n",
    "def load_dataset(path: str = data_file_path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the dataset from a specified parquet file.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the parquet file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The loaded DataFrame, or an empty DataFrame if the file is not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_parquet(path)\n",
    "        print(f\"✅ Dataset loaded successfully from {path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: Dataset not found at {path}. Please ensure the file exists and the path is correct.\")\n",
    "        return pd.DataFrame() # Return an empty DataFrame to indicate failure\n",
    "\n",
    "# --- Data Preprocessing Functions ---\n",
    "def train_test_impute_split(df: pd.DataFrame, label_col: str = \"mortality_30d\") -> tuple:\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets, and imputes missing values\n",
    "    using medians calculated from the training set.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        label_col (str): The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "        tuple: X_train, X_test, y_train, y_test DataFrames.\n",
    "    \"\"\"\n",
    "    # Drop identifiers and timestamps\n",
    "    df = df.drop(columns=['icustay_id', 'subject_id', 'hadm_id', 'admittime', 'dob', 'dod', 'intime', 'outtime', 'icd9_code'], errors='ignore')\n",
    "\n",
    "    y = df[label_col]\n",
    "    X = df.drop(columns=[label_col])\n",
    "\n",
    "    # Split before imputation to prevent data leakage\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Impute missing values using training set statistics only\n",
    "    for col in X_train.select_dtypes(include=np.number).columns:\n",
    "        if X_train[col].isnull().any():\n",
    "            median_val = X_train[col].median()\n",
    "            X_train[col] = X_train[col].fillna(median_val)\n",
    "            X_test[col] = X_test[col].fillna(median_val)\n",
    "    \n",
    "    print(\"✅ Data split and imputed successfully.\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def check_for_leakage(X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Checks for potential data leakage by identifying highly correlated features\n",
    "    with the target variable. Drops highly correlated columns if detected.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Features DataFrame.\n",
    "        y (pd.Series): Target Series.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Features DataFrame after dropping highly correlated columns\n",
    "                      if leakage is detected.\n",
    "    \"\"\"\n",
    "    X_copy = X.copy()\n",
    "    y_copy = y.copy()\n",
    "    # Reset indices to ensure correct concatenation for correlation calculation\n",
    "    X_copy.index = range(len(X_copy))\n",
    "    y_copy.index = range(len(y_copy))\n",
    "\n",
    "    combined_df = pd.concat([X_copy, y_copy], axis=1)\n",
    "    \n",
    "    # Ensure all columns used for correlation are numeric\n",
    "    numeric_cols = combined_df.select_dtypes(include=np.number).columns\n",
    "    combined_df_numeric = combined_df[numeric_cols]\n",
    "\n",
    "    if y.name not in combined_df_numeric.columns:\n",
    "        print(f\"⚠️ Warning: Target column '{y.name}' not found in numeric columns for leakage check. Skipping correlation check.\")\n",
    "        return X\n",
    "\n",
    "    corr = combined_df_numeric.corr()[y.name].drop(y.name, errors='ignore') # drop target if it's there\n",
    "    high_corr_threshold = 0.95\n",
    "    high_corr = corr[abs(corr) > high_corr_threshold]\n",
    "\n",
    "    if not high_corr.empty:\n",
    "        print(f\"\\n⚠️ Potential Leakage Detected (correlation > {high_corr_threshold}):\")\n",
    "        print(high_corr)\n",
    "        leaky_columns = high_corr.index.tolist()\n",
    "        X = X.drop(columns=leaky_columns, errors='ignore')\n",
    "        print(f\"Dropped potential leakage columns: {leaky_columns}\")\n",
    "    else:\n",
    "        print(\"\\nNo significant data leakage detected based on high correlation.\")\n",
    "    return X\n",
    "\n",
    "# --- Model Training and Evaluation ---\n",
    "def train_logistic_regression(X_train: np.ndarray, y_train: pd.Series, \n",
    "                              X_test: np.ndarray, y_test: pd.Series) -> tuple:\n",
    "    \"\"\"\n",
    "    Trains a Logistic Regression model and evaluates its performance.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Scaled training features.\n",
    "        y_train (pd.Series): Training labels.\n",
    "        X_test (np.ndarray): Scaled testing features.\n",
    "        y_test (pd.Series): Testing labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Trained model, X_train, y_train, X_test, y_test, y_pred, y_prob.\n",
    "    \"\"\"\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(\"\\n🧠 Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n📊 ROC AUC Score:\", roc_auc_score(y_test, y_prob))\n",
    "    print(\"✅ Logistic Regression model trained and evaluated.\")\n",
    "    return model, X_train, y_train, X_test, y_test, y_pred, y_prob\n",
    "\n",
    "# --- SHAP Explanation and Plotting ---\n",
    "def explain_single_patient_prediction(model, patient_data_scaled: np.ndarray, patient_data_df: pd.DataFrame, \n",
    "                                      output_dir: str, background_data_scaled: np.ndarray, patient_id: str = \"single_patient\"):\n",
    "    \"\"\"\n",
    "    Generates a SHAP waterfall plot for a single patient's prediction.\n",
    "\n",
    "    Args:\n",
    "        model: The trained machine learning model.\n",
    "        patient_data_scaled (np.ndarray): The scaled feature data for the single patient.\n",
    "                                          Expected shape (1, n_features).\n",
    "        patient_data_df (pd.DataFrame): The original (unscaled) feature data for the single patient,\n",
    "                                        as a DataFrame. Used for feature names and context.\n",
    "                                        Expected shape (1, n_features).\n",
    "        output_dir (str): Directory to save the SHAP plot.\n",
    "        background_data_scaled (np.ndarray): A representative background dataset (e.g., a sample of the\n",
    "                                             scaled training data) for the explainer.\n",
    "        patient_id (str): An identifier for the patient, used in the plot title and filename.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if patient_data_scaled.shape[0] != 1:\n",
    "        print(\"Error: explain_single_patient_prediction expects data for a single patient.\")\n",
    "        return\n",
    "\n",
    "    # Use a representative background dataset for the explainer\n",
    "    # For Logistic Regression, LinearExplainer is efficient and accurate.\n",
    "    # The background data should be from the training set.\n",
    "    explainer = shap.LinearExplainer(model, background_data_scaled)\n",
    "    shap_values = explainer.shap_values(patient_data_scaled)\n",
    "\n",
    "    # SHAP waterfall expects a single Explanation object\n",
    "    single_expl = shap.Explanation(\n",
    "        values=shap_values[0],  # For a single prediction, shap_values is an array of arrays\n",
    "        base_values=explainer.expected_value,\n",
    "        data=patient_data_df.iloc[0].values,\n",
    "        feature_names=patient_data_df.columns.tolist()\n",
    "    )\n",
    "    \n",
    "    shap.plots.waterfall(single_expl, show=False)\n",
    "    plt.title(f\"SHAP Waterfall - {patient_id}\")\n",
    "    plt.savefig(os.path.join(output_dir, f\"waterfall_{patient_id}.png\"), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ SHAP waterfall plot for {patient_id} saved.\")\n",
    "\n",
    "\n",
    "def explain_predictions_batch(model, X_scaled: np.ndarray, X_df: pd.DataFrame, \n",
    "                              output_dir: str = \"shap_plots\", top_n: int = 10):\n",
    "    \"\"\"\n",
    "    Generates SHAP plots for overall feature importance and top N high-risk patients.\n",
    "    Suitable for Streamlit apps — saves figures as PNGs for display.\n",
    "    This function is for batch explanation.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if X_scaled.shape[0] == 0:\n",
    "        print(\"Skipping SHAP explanation: No data in X_scaled.\")\n",
    "        return\n",
    "\n",
    "    # SHAP values via KernelExplainer or LinearExplainer depending on model\n",
    "    # For a general explainer, using the model directly is often the most robust.\n",
    "    # For Logistic Regression, we could use LinearExplainer with X_scaled as background if this were a batch explanation for the *entire* test set\n",
    "    # but here we use the generic shap.Explainer for simplicity which handles model types.\n",
    "    explainer = shap.Explainer(model, X_df) # X_df is used here to retain feature names\n",
    "    shap_values = explainer(X_df)\n",
    "\n",
    "    # Summary Plot (Overall)\n",
    "    shap.summary_plot(shap_values, features=X_df, show=False)\n",
    "    plt.savefig(os.path.join(output_dir, \"shap_summary_overall.png\"), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✅ SHAP summary plot saved.\")\n",
    "\n",
    "    # Dependence plots for top features\n",
    "    # Ensure shap_values.values is not empty before attempting to access it\n",
    "    if shap_values.values.size > 0:\n",
    "        abs_mean = np.abs(shap_values.values).mean(axis=0)\n",
    "        # Handle cases where number of features might be less than top_n\n",
    "        top_feats_indices = np.argsort(abs_mean)[::-1][:min(top_n, len(X_df.columns))]\n",
    "        \n",
    "        for idx in top_feats_indices:\n",
    "            feat = X_df.columns[idx]\n",
    "            shap.dependence_plot(feat, shap_values.values, X_df, show=False)\n",
    "            plt.title(f\"SHAP Dependence: {feat}\")\n",
    "            plt.savefig(os.path.join(output_dir, f\"dependence_{feat}.png\"), bbox_inches='tight')\n",
    "            plt.close()\n",
    "        print(\"✅ SHAP dependence plots saved.\")\n",
    "    else:\n",
    "        print(\"No SHAP values to plot for dependence plots.\")\n",
    "\n",
    "\n",
    "    # Get top N high-risk patients (by predicted probability)\n",
    "    # Ensure X_scaled is not empty before predicting probabilities\n",
    "    if X_scaled.shape[0] > 0:\n",
    "        risk_scores = model.predict_proba(X_scaled)[:, 1]\n",
    "        top_indices = np.argsort(risk_scores)[-top_n:][::-1]\n",
    "\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            pid = f\"patient_{idx}_rank_{i+1}\"\n",
    "            # SHAP waterfall expects a single Explanation object\n",
    "            single_expl = shap.Explanation(\n",
    "                values=shap_values.values[idx],\n",
    "                base_values=shap_values.base_values[idx],\n",
    "                data=X_df.iloc[idx],\n",
    "                feature_names=X_df.columns\n",
    "            )\n",
    "            shap.plots.waterfall(single_expl, show=False)\n",
    "            plt.title(f\"SHAP Waterfall - {pid}\")\n",
    "            plt.savefig(os.path.join(output_dir, f\"waterfall_{pid}.png\"), bbox_inches='tight')\n",
    "            plt.close()\n",
    "        print(f\"✅ SHAP waterfall plots for top {top_n} patients saved.\")\n",
    "    else:\n",
    "        print(\"No patients to generate waterfall plots for.\")\n",
    "\n",
    "    return shap_values\n",
    "\n",
    "# --- Model Saving Function ---\n",
    "def save_model(model, scaler, output_path: str):\n",
    "    \"\"\"\n",
    "    Saves the trained model and scaler to a joblib file.\n",
    "\n",
    "    Args:\n",
    "        model: The trained machine learning model.\n",
    "        scaler: The fitted StandardScaler object.\n",
    "        output_path (str): The path to save the model.\n",
    "    \"\"\"\n",
    "    joblib.dump({\"model\": model, \"scaler\": scaler}, output_path)\n",
    "    print(f\"\\n✅ Saved model and scaler to {output_path}\")\n",
    "\n",
    "# --- Main MLflow Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up MLflow tracking\n",
    "    mlflow.set_experiment(\"OncoAI-Mortality-Prediction\")\n",
    "\n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run() as run:\n",
    "        run_id = run.info.run_id\n",
    "        \n",
    "        # Make SHAP plots output directory specific to the run\n",
    "        run_shap_output_dir = os.path.join(shap_plots_base_path, run_id)\n",
    "        os.makedirs(run_shap_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Define the full model save path for this run\n",
    "        model_save_path_for_run = os.path.join(model_save_base_path, f\"logreg_model_run_{run_id}.joblib\")\n",
    "\n",
    "        print(f\"Starting MLflow Run with ID: {run_id}\")\n",
    "        print(f\"SHAP plots will be saved to: {run_shap_output_dir}\")\n",
    "        print(f\"Model will be saved to: {model_save_path_for_run}\")\n",
    "\n",
    "        # 1. Load dataset\n",
    "        df = load_dataset()\n",
    "\n",
    "        if df.empty: # Check if DataFrame is empty\n",
    "            print(\"❌ Dataset is empty. Cannot proceed with training and explanation. Exiting MLflow run.\")\n",
    "            mlflow.end_run(status=\"FAILED\")\n",
    "        else:\n",
    "            # 2. Train-test split and imputation\n",
    "            X_train, X_test, y_train, y_test = train_test_impute_split(df)\n",
    "\n",
    "            # 3. One-hot encode categorical columns\n",
    "            X_train_ohe = pd.get_dummies(X_train, drop_first=True)\n",
    "            X_test_ohe = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "            # Align columns after one-hot encoding to ensure same features in train/test\n",
    "            missing_cols_in_test = set(X_train_ohe.columns) - set(X_test_ohe.columns)\n",
    "            for c in missing_cols_in_test:\n",
    "                X_test_ohe[c] = 0\n",
    "            # Ensure the order of columns is the same\n",
    "            X_test_ohe = X_test_ohe[X_train_ohe.columns]\n",
    "\n",
    "            # 4. Check for data leakage on the one-hot encoded training data\n",
    "            X_train_leakage_checked = check_for_leakage(X_train_ohe, y_train)\n",
    "\n",
    "            # Apply the same column selection (after leakage check) to the test set\n",
    "            X_test_leakage_checked = X_test_ohe[X_train_leakage_checked.columns]\n",
    "\n",
    "            # DEBUG: Print column names to verify before scaling and SHAP\n",
    "            print(f\"\\nDEBUG: Columns of X_train_leakage_checked before scaling:\\n{X_train_leakage_checked.columns.tolist()}\")\n",
    "            print(f\"DEBUG: Columns of X_test_leakage_checked before scaling:\\n{X_test_leakage_checked.columns.tolist()}\")\n",
    "            \n",
    "            # 5. Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train_leakage_checked)\n",
    "            X_test_scaled = scaler.transform(X_test_leakage_checked)\n",
    "\n",
    "            print(\"✅ Features prepared (one-hot encoded and scaled).\")\n",
    "\n",
    "            # Log parameters to MLflow\n",
    "            mlflow.log_param(\"scaler\", \"StandardScaler\")\n",
    "            mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "\n",
    "            # 6. Train Logistic Regression model\n",
    "            model, X_train_final_scaled, y_train_final, X_test_final_scaled, y_test_final, y_pred, y_prob = \\\n",
    "                train_logistic_regression(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "            # Log ROC AUC metric to MLflow\n",
    "            auc = roc_auc_score(y_test_final, y_prob)\n",
    "            mlflow.log_metric(\"roc_auc\", auc)\n",
    "\n",
    "            # 7. Save model and scaler\n",
    "            save_model(model, scaler, output_path=model_save_path_for_run)\n",
    "\n",
    "            # 8. Log model with input_example for signature inference\n",
    "            if X_train_leakage_checked.shape[0] > 0:\n",
    "                mlflow.sklearn.log_model(model, \"logreg_model\", \n",
    "                                         input_example=X_train_leakage_checked.head(10))\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(model, \"logreg_model\") \n",
    "\n",
    "            # 9. Explain with SHAP for batch (overall and top N high-risk patients)\n",
    "            # This function remains for overall model explanation and high-risk patients.\n",
    "            explain_predictions_batch(model, X_test_scaled, X_test_leakage_checked, output_dir=run_shap_output_dir, top_n=10)\n",
    "\n",
    "            # NEW: Explain with SHAP for a single patient\n",
    "            # Select an arbitrary patient from the test set for demonstration\n",
    "            if X_test_scaled.shape[0] > 0:\n",
    "                single_patient_idx = 0  # Explain the first patient in the test set\n",
    "                single_patient_scaled_data = X_test_scaled[single_patient_idx].reshape(1, -1)\n",
    "                single_patient_df = X_test_leakage_checked.iloc[[single_patient_idx]]\n",
    "                \n",
    "                # Pass a sample of the training data as background for the explainer\n",
    "                # It's good practice to use a smaller, representative sample for performance.\n",
    "                # For LinearExplainer, the mean of the training data is also a valid background.\n",
    "                # Here, we'll use a small sample of X_train_scaled.\n",
    "                background_sample_size = min(100, X_train_scaled.shape[0]) # Use up to 100 samples\n",
    "                background_data_for_shap = X_train_scaled[np.random.choice(X_train_scaled.shape[0], background_sample_size, replace=False)]\n",
    "\n",
    "                explain_single_patient_prediction(model, single_patient_scaled_data, \n",
    "                                                  single_patient_df, run_shap_output_dir, \n",
    "                                                  background_data_for_shap, # Pass the background data\n",
    "                                                  patient_id=f\"test_patient_{single_patient_idx}\")\n",
    "            else:\n",
    "                print(\"No test data available to explain a single patient.\")\n",
    "\n",
    "            # 10. Log SHAP plots as MLflow artifacts\n",
    "            shap_plot_files = [f for f in os.listdir(run_shap_output_dir) if f.endswith('.png')]\n",
    "            for plot_file in shap_plot_files:\n",
    "                mlflow.log_artifact(os.path.join(run_shap_output_dir, plot_file), artifact_path=\"shap_plots\")\n",
    "\n",
    "    print(\"\\n✨ MLflow run completed successfully. Check your MLflow UI for details.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b311a-9d98-465e-84a5-de2d969d5f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64cc7e-99d5-4c7a-a071-5fe7e37f3ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
